---
title: "ISLR - Lab 09"
author: "Saad M. Siddiqui"
date: "4/10/2022"
output: 
  html_document:
    toc: TRUE 
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ISLR2)
library(e1071)        # For SVM API
library(ROCR)         # For ROC curves 

library(dplyr)
library(ggplot2)
```

# Example 01 - Support Vector Classifier 
## First Classifier 
The `e1071` library contains implementations of multiple statistical learning methods, including support vector machines which can be trained using `svm`. The backend implementation of this `svm` method is slightly different from the one we have covered in the chapter. It uses a `cost` parameter to specify the penalty for a violation of the margin. When `cost` is small, margins will be wide because margin violations don't incur a large loss penalty. When `cost` is large, margins will be small because margin violations incur a large loss penalty. This also leads to fewer support vectors on the margin or violating the margin.

We use the `svm` function with a `linear` kernel on a simulated data set and check if they are linearly separable. 

```{r}
set.seed(1)
x <- matrix(rnorm(20 * 2), ncol = 2)    # 20 observations, 2 predictors 
y <- c(rep(-1, 10), rep(1, 10))         # Half belong to each class
x[y == 1, ] <- x[y == 1, ] + 1          # We offset each predictor for a class by 1
plot(x, col = (3 - y))                  # col (3 - y) = [3 - 1 = 2, 3 + 1 = 4]
```
We see that the points are in fact not linearly separable. We now fit the support vector classifier to this data, converting the response to a factor. The `scale` argument tells `svm` not to scale each feature to have mean 0 and variance of 1. 
```{r}
svm.data <- data.frame(x = x, y = as.factor(y)) 
svm.fit <- svm(y ~., data = svm.data, kernel = 'linear', cost = 10, scale = FALSE)
```

We now plot the support vectors and margin of our `svm.fit` object to see if it does a good job of separating the data - expectation is that it doesn't. 
```{r}
plot(svm.fit, svm.data)
```

The decision boundary is actually linear but appears a bit jagged because of the way the data is rendered. 

Support vectors are marked with crosses and non-support vector data points are marked with 0s. 

The color coding shows a clear decision boundary which is linear (because the kernel was linear, after all) and that is violated in certain situations: e.g. there are two support vectors that lie on the decision boundary. 

We can show the indexes of the data points that have been flagged as support vectors. 
```{r}
svm.fit$index
```

This shows that there are in fact 7 support vectors in the data set. 

We can obtain some basic information about the SVM fit using the `summary` command.
```{r}
summary(svm.fit)
```

This tells us that there are 4 support vectors for class -1 and three vectrs for class +1. 

## Adjusting Cost
What if we adjusted the cost to be smaller? We expect more margin violations and more support vectors.
```{r}
svm.fit.lower.cost <- svm(y ~., data = svm.data, kernel = 'linear', cost = 0.1, 
                          scale = FALSE)
plot(svm.fit.lower.cost, svm.data)
svm.fit.lower.cost$index
summary(svm.fit.lower.cost)
```

## Tuning the Cost 
The `e1071` library includes a built-in `tune` method to perform cross-validation of `svm` and other models within the package. By default, it uses 10 folds. We pass a `list` of all hyperparameters that we want to tune as well as a set of candidate values to test for each hyperparameter.
```{r}
set.seed(1)

# We pass the **FUNCTION** used to fit the model, not the model itself
# So the argument is `svm` method, not `svm.fit` or `svm.fit.lower.cost`
svm.tune.out <- tune(svm, y ~ ., data = svm.data, kernel = 'linear',
                     ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))

summary(svm.tune.out)
```

We can see that `cost = 0.1` is found to be the optimal value (among the set of values we tested). The `tune` method also stores the best model obtained, which can then be accessed and used to make predictions.
```{r}
# Get the best predictor from CV object
svm.fit.best.cost <- svm.tune.out$best.model
summary(svm.fit.best.cost)

# Run predictions
x.test <- matrix(rnorm(20 * 2), ncol = 2)
y.test <- sample(c(-1, 1), 20, rep = TRUE) 
x.test[y.test == 1, ] <- x.test[y.test == 1] + 1 
svm.test.data <- data.frame(x = x.test, y = as.factor(y.test))

svm.pred.best.cost <- predict(svm.fit.best.cost, svm.test.data)
table(predict = svm.pred.best.cost, truth = svm.test.data$y)
```

Thus, with a cost of 0.1, 17 of the 20 test observations are classified correctly. 

## More Cost Experiments
How does this result change if we use `cost = 0.01`?
```{r}
svm.fit.cost_0_01 <- svm(y ~., data = svm.data, kernel = 'linear', cost = 0.01, 
                         scale = FALSE)
svm.pred.cost_0_01 <- predict(svm.fit.cost_0_01, svm.test.data)
table(predict = svm.pred.cost_0_01, truth = svm.test.data$y)

```

In this case, 3 additional data points are misclassified. 

## Linearly Separable Data
We modify the training data so that the two classes are just barely linearly separable. With this modification in place, we can set the `cost` to a very high value to ensure no margin violatons.
```{r}
x[y == 1, ] <- x[y == 1, ] + 0.5 
plot(x, col = (y + 5) / 2, pch = 19)
```

```{r}
svm.data <- data.frame(x = x, y = as.factor(y)) 
svm.fit.higher.cost <- svm(y ~ ., data = svm.data, kernel = 'linear', cost = 1e5)
summary(svm.fit.higher.cost)
plot(svm.fit.higher.cost, data = svm.data)
```

We can see that the two classes are now perfectly linearly separable, with no margin violations i.e. no training errors. however, the margin is definitely very narrow - we can tell this is the case because the non-support vector data points marked with `o` are very close the decision boundary. Since we expect this model to perform poorly on test data, we can readjust the cost to a lower value.
```{r}
svm.fit.high.cost <- svm(y ~., data = svm.data, kernel = 'linear', cost = 1)
summary(svm.fit.high.cost)
plot(svm.fit.high.cost, data = svm.test.data)
```
With this cost, we do misclassify two observations. However, it is likely that this model will peform better on test data because the margin is somewhat wider. 

# Example 02 - Support Vector Machine 
## Radial SVM - First Attempt
We now fit a variant of the support vector classifier that uses a non-linear kernel i.e. a Support Vector Machine. We can do so with the `svm` function, and use the `kernel` argument to specify if we want to use a `polynomial` kernel or a `radial` kernel, and then specify the `degree` or `gamma` hyperparameters depending on which kernel we choose. 

First, we generate some training data that has a non-linear boundary.
```{r}
set.seed(1)

# Matrix of two features
x <- matrix(rnorm(200 * 2), ncol = 2) 

# Offset the features so that the decision boundary becomes non-linear
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150, ] - 2
y <- c(rep(1, 50), rep(2, 50))

# Plotting the data makes it clear that these classes are not linearly separable
plot(x, col = y)

svm.data <- data.frame(
  x = x, y = as.factor(y)
)
```

Split the data into a train and test set randomly, and then fit a support vector machine with a radial basis function as the kernel.
```{r}
svm.train.idx <- sample(200, 100)   # Half of the data for training
kernel.svm.radial <- svm(y ~., data = svm.data[svm.train.idx, ], kernel = 'radial',
                         gamma = 1, cost = 1)
plot(kernel.svm.radial, svm.data[svm.train.idx, ])
summary(kernel.svm.radial)
```

Analysis of the plot

- Firstly, we see that the plot has zoomed into a region of the feature space with the support vectors, and has not shown the entire space. 

- Secondly, we see that the decision boundary is clearly non-linear, and there all non-support vector data points are quite fart from the decision boundary.

- There is evidence of at least one data point that is marked as a margin violation near $(x_1, x_2) \approx (0.9, -0.5)$.

- Moreover, the idea is that in a Gaussian kernel-based transformation of this feature space, this decision boundary will be linear.

## Radial SVM - Second Attempt
Because we saw some margin violations in the previous plot, we'll repeat the fitting with a higher cost parameter.
```{r}
kernel.svm.radial.higher.cost <- svm(
  y ~., data = svm.data[svm.train.idx, ], kernel = 'radial', gamma = 1, cost = 1e5
)
plot(kernel.svm.radial.higher.cost, svm.data[svm.train.idx, ])
```

We can perform cross-validation using `tune` to select the best choice of $\gamma$ and `cost` for an SVM with a radial kernel.
```{r}
set.seed(1)
kernel.svm.radial.tune <- tune(
  svm, 
  y ~ ., 
  data = svm.data[svm.train.idx, ], 
  kernel = 'radial', 
  ranges = list(
    cost = c(0.1, 1, 10, 100, 1000),
    gamma = c(0.5, 1, 2, 3)
  )
)

summary(kernel.svm.radial.tune)
```

The best choice of hyperparameters is `cost = 1`, `gamma = 1`. 

Test the predictions for this model by applying the `predict` function to it.
```{r}
table(
  true = svm.data[-svm.train.idx, "y"], 
  pred = predict(
    kernel.svm.radial.tune$best.model, newdata = svm.data[-svm.train.idx,]
  )
)

mean(
  svm.data[-svm.train.idx, "y"] != 
    predict(kernel.svm.radial.tune$best.model, newdata = svm.data[-svm.train.idx,])
)
```

29% of the test observations are misclassified by this SVM.


# Example 03 - ROC Curves 
We can use the `ROCR` package to produce ROC curves for classification models, including  SVMs. 

We first write a smple function to plot a ROC curve given a vector containing a numerical score for each observation `pred` and a vector containing the class label for each observation `truth`.
```{r}
roc_plot <- function(pred, truth, ...) {
  pred.ob <- prediction(pred, truth) 
  perf <- performance(pred.ob, "tpr", "fpr")
  plot(perf, ...)
}
```

By default, SVM and support vector classifiers output the predicted class labels for each data point. However, we know that distance of a data point from a support vector can also be used as a proxy for how certain the algorithm is that a data point belongs to a particular class. We can also have the `svm` API return the actual fitted values
$$
f(X^*) = \beta_0 + \beta_1X_1^* + \beta_2X_2^* + ... + \beta_jX_j^*
$$
where the sign of $f(X*)$ tells determines the side of the decision boundary that the data point lies on. If this fitted value exceeds 0, then the observation is assigned to class 1. Otherwise, it is assigned to class 0. 

```{r}
svm.fit.opt <- svm(y ~ ., data = svm.data[svm.train.idx, ], 
                   kernel = 'radial', gamma = 2, cost = 1, decision.values = T)

svm.fitted.values <- attributes(
  predict(svm.fit.opt, svm.data[svm.train.idx, ], decision.values = TRUE)
)$decision.values

svm.fitted.values
```

We can now produce the ROC plot. We use the negative of the fitted values so that the negative values correspond to class 1 and positive valuse to class 2. 
```{r}
par(mfrow = c(1, 2))
roc_plot(
  -1 * svm.fitted.values, svm.data[svm.train.idx, "y"], 
  main = 'Training Data'
)

# Repeat the process for a different gamma that will make a more flexible fit 
svm.fit.flex <- svm(y ~., data = svm.data[svm.train.idx, ], 
                    kernel = 'radial', gamma = 50, cost = 1, 
                    decision.values = T)
fitted <- attributes(
  predict(svm.fit.flex, svm.data[svm.train.idx, ], decision.values = T)
)$decision.values

roc_plot(-1 * fitted, svm.data[svm.train.idx, "y"], add = T, col = 'red')

# Repeat the same process on test data 
fitted <- attributes(
  predict(svm.fit.opt, svm.data[-svm.train.idx, ], decision.values = T)
)$decision.values
roc_plot(-1 * fitted, svm.data[-svm.train.idx, "y"], main = "Test Data") 

fitted <- attributes(
  predict(svm.fit.flex, svm.data[-svm.train.idx, ], decision.values = T)
)$decision.values 
roc_plot(-1 * fitted, svm.data[-svm.train.idx, "y"], add = T, col = 'red')
```

So a $\gamma$ of 50 definitely overfits the training data and results in significantly worse test set performance.

# Example 04 - SVM with Multiple Classes
If the response is a factor containing more than two levels, then the `svm` function automatically performs multi-class classification using the **one-versus-one approach**. 
```{r}
# set.seed(1) 
# x <- rbind(x, matrix(rnorm(50 * 2), ncol = 2))
# y <- c(y, rep(0, 50)) 
# 
# x[y == 0, 2] <- x[y == 0, 2] + 2 
# svm.data <- data.frame(x = x, y = as.factor(y)) 
# par(mfrow = c(1, 1))
# plot(x, col = (y + 1)) 
# 
# svm.fit <- svm(y ~., data = svm.data, kernel = 'radial', cost = 10, gamma = 1)
# plot(svm.fit, svm.data)
```

# Example 05 - Gene Expression Data
We apply the SVM to the `Khan` gene expression data set, which consists of a number of tissue samples corresponding to four distinct types of small round blue cell tumours. 

For each tissue sample, gene expression measurements are available. 

This data set has already been segmented into `xtrain`, `ytrain` and `xtest`, `ytest`.

```{r}
# Examine data set dimensions
names(Khan)

# Dimensions of training data 
dim(Khan$xtrain)
length(Khan$ytest)

# Dimensions of testing data
dim(Khan$xtest)
length(Khan$ytest)
```

Data set contains gene expression measurement for 2308 genes and the training and test sets are 63 and 20 observations. 

We use a support vector approach to predict cancer subtype using gene expression measurements. Because the number of features $p$ is significantly larger than the number of observations $n$, this suggests we should use a **linear kernel**. The additional flexibility that will result from using a polynomial or radial kernel is unnecessary, because we already have a high chance of overfitting given the dimensionality.

```{r}
gene.data.train <- data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain))
gene.svm.out <- svm(y ~., data = gene.data.train, kernel = 'linear', cost = 10)
summary(gene.svm.out)
table(gene.svm.out$fitted, gene.data.train$y)
```

We see that there are no errors in the training data, which suggests we are most likely overfitting due to the large dimensionality of the data set, which makes it easir to find hyperplanes that fully separate the classes. 

We are interested in the classifier's performance on the test observations.
```{r}
gene.data.test <- data.frame(x = Khan$xtest, y = as.factor(Khan$ytest))
gene.pred.test <- predict(gene.svm.out, newdata = gene.data.test) 
table(gene.pred.test, gene.data.test$y)
```

We see that a `cost` of 10 results in 2 errors on this data set.